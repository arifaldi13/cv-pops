# The Present-Day CV Population

CV population material in [Putra+25}(https://gcs.itb.ac.id/proceeding-igsc/igsc/article/view/289)

## The Database

The database is in the folder 'MESA_COMPILED' which consists of a compiled MESA output with $M_1$, $M_2$, and $P_\text{i}$ as the initial parameters and $P_\text{f}$, $P_\text{WD}$ and the status of mass transfer that in the threshold of CV at the time of given age. Each txt file represents the "age window". The list of corresponding ages is in the _legend.txt file. The value of simulation results consists of an inf value, which means the model already 'dead' at the age window.

## Present-Day CV Data

The data is in the folder "POPS" which consists of the synthesized present-day CV population. This is the result of interpolation of the database with the grid. It has the initial parameters and values ($M_1$, $M_2$, and $P_\text{i}$) and the corresponding interpolated values $P_\text{f}$ and $P_\text{WD}$. Each txt file represents the "age window". The list of corresponding ages is in the _legend.txt file.

## Cataclysmic Variable (CV) Population Synthesis with Machine Learning

This project explores the formation and population of Cataclysmic Variables (CVs) by leveraging machine learning to create a surrogate model for complex astrophysical simulations. The core of this repository is a deep learning model trained on data from the **Modules for Experiments in Stellar Astrophysics (MESA)** code.

### Project Motivation

The evolution of binary star systems is governed by complex physical laws. Simulating this evolution using numerical codes like MESA is scientifically powerful but computationally expensive, requiring significant time and resources for large-scale population studies.

This project aims to bridge this gap by training a neural network to replicate the simulation's output. By learning the relationship between a binary system's initial parameters (ZAMS masses and orbital period) and its likelihood of becoming a CV at a given age, the machine learning model can serve as a fast and efficient emulator, enabling rapid exploration of the parameter space.

### Analysis Notebook: `CV_Classification_MESA.ipynb`

A key component of this project is the Jupyter Notebook that details the end-to-end process of building, training, and evaluating the CV classification model.

#### Overview

The notebook provides a structured guide for classifying binary systems as CVs or non-CVs. It uses a dataset generated by a grid of MESA simulations and builds a neural network with TensorFlow to learn the classification function.

#### Key Features of the Notebook:

*   **Data Preprocessing**: Loads and structures the MESA simulation data, which is distributed across multiple files corresponding to different evolutionary ages.
*   **Feature Engineering**: Selects the key input features for the model:
    1.  Evolutionary Age
    2.  Initial Primary Mass (M1)
    3.  Initial Secondary Mass (M2)
    4.  Initial Orbital Period (Pi)
*   **Handling Class Imbalance**: Effectively addresses the physical reality that CVs are rare. The notebook visualizes the class imbalance and uses `class_weight` to ensure the model learns to identify the minority class.
*   **Feature Normalization**: Implements `tf.keras.layers.Normalization` to scale features of vastly different magnitudes (e.g., age in Gyr vs. mass in Mâ˜‰), leading to stable and efficient training.
*   **Robust Model Training**: Employs a validation set and `EarlyStopping` to prevent overfitting and ensure the model generalizes well to unseen data.
*   **In-depth Evaluation**: Uses a `classification_report` to assess model performance, paying close attention to precision and recall for the minority CV class.
*   **Insightful Visualization**: The final section features a powerful three-panel plot that visualizes the model's performance by comparing the original MESA data, the model's predictions, and the learned decision boundary, providing deep insight into the model's behavior.

#### Technologies Used

*   **Python 3**
*   **Jupyter Notebook**
*   **TensorFlow / Keras**: For building and training the neural network.
*   **Scikit-learn**: For data splitting and evaluation metrics.
*   **Pandas**: For data manipulation.
*   **NumPy**: For numerical operations.
*   **Matplotlib**: For visualization.


### Getting Started

#### Prerequisites

Ensure you have Python 3 and the required libraries installed. You can install them using pip:

```bash
pip install tensorflow pandas numpy scikit-learn matplotlib tqdm
```

#### Usage

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/arifaldi13/cv-pops.git
    cd cv-pops
    ```
2.  **Run the notebook:** Launch Jupyter Notebook and open `CV_Classification_MESA.ipynb` (or your chosen notebook name) to see the full analysis, training process, and results.


### Using the Pre-Trained Model for Predictions

Once the model has been trained and saved (as done in the notebook), it can be loaded directly for quick inference without needing to retrain. This is the primary advantage of creating a surrogate model.

#### Example Script

Here is a simple Python script demonstrating how to load the model and predict whether a set of binary systems will become CVs.

```python
import tensorflow as tf
import numpy as np

# 1. Load the trained model
# The model includes the architecture, weights, and the normalization layer.
model_path = './saved_model/cv_classification'
model = tf.keras.models.load_model(model_path)

# 2. Define new data for prediction
# Each row is a system with [age, M1, M2, Pi]
# Example: Predict the outcome for two systems at an age of 6.6 Gyr
new_data = np.array([
    # System 1: age=6.6e9, M1=1.3, M2=0.20, Pi=150 days
    [6.6e9, 1.3, 0.20, 150.0],
    
    # System 2: age=6.6e9, M1=1.3, M2=0.35, Pi=300 days
    [6.6e9, 1.3, 0.35, 300.0]
])

# 3. Get model predictions
# The model outputs raw logits, so we apply a sigmoid function
raw_predictions = model.predict(new_data)
probabilities = tf.nn.sigmoid(raw_predictions).numpy().flatten()

# 4. Interpret the results
# A probability > 0.5 suggests the system is a CV
threshold = 0.5
classifications = ["CV" if p >= threshold else "Not a CV" for p in probabilities]

for i, system in enumerate(new_data):
    print(f"System: {system}")
    print(f"  -> CV Likelihood: {probabilities[i]:.4f}")
    print(f"  -> Classification: {classifications[i]}\n")
```

This script shows how the saved model can be used as a standalone tool for rapid prediction, fulfilling the project's primary goal.

### Model Performance and Results

The model performs exceptionally well at replicating the MESA simulation outputs. The key results from the notebook are:

*   **High Overall Accuracy**: The model achieves an overall accuracy of approximately **97.7%** on the evaluation dataset.
*   **Excellent Minority Class Recall**: Despite the class imbalance, the model correctly identifies over **98%** of the true CV systems (high recall). This is crucial for the model's scientific utility, as we want to avoid missing these rare events.
*   **Learned Decision Boundary**: Visualization confirms that the model has not just memorized the data but has learned a smooth and physically plausible decision boundary that separates CVs from non-CVs in the parameter space.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgments

This work is based on a grid of simulations produced by the **MESA (Modules for Experiments in Stellar Astrophysics)** code. We gratefully acknowledge the developers of this invaluable tool.
    
